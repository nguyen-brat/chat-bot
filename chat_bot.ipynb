{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen-brat/chat-bot/blob/main/chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83SM2n2RluKu"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5KruY8jDxqxz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "neGpMnG89Byj"
      },
      "outputs": [],
      "source": [
        "mask_percent = 0.15 # percent data replace with mask\n",
        "train_size = 0.8\n",
        "valid_size = 0.1\n",
        "test_size = 0.1\n",
        "max_len = 512 # max len of input feed to neuron network\n",
        "max_dim = 128 # world embedding dimention\n",
        "batch_size = 32\n",
        "num_epochs = 5\n",
        "n_blocks = 1 # n transformer block\n",
        "lr = 0.01 # learning rate\n",
        "n_heads = 8 # n_head in transformer block\n",
        "drop_rate = 0.1 # drop rate in transformer block\n",
        "freq_word = 30 # remove all world below freq_word\n",
        "d_dim = 128 # dimention in FFN layers\n",
        "vocab_size = 30000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoIugp8mzvvB",
        "outputId": "9c63b825-5881-41d1-9cc2-23e532818a0d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UalT1_HhzzkX",
        "outputId": "aca6344f-aa0f-45b1-8436-53d78d6a7572"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>message</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you a fan of Google or Microsoft?</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Both are excellent technology they are helpfu...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I'm not  a huge fan of Google, but I use it a...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Google provides online related services and p...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Yeah, their services are good. I'm just not a...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188373</th>\n",
              "      <td>8628</td>\n",
              "      <td>Wow, it does not seem like that long. Since I...</td>\n",
              "      <td>Surprised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188374</th>\n",
              "      <td>8628</td>\n",
              "      <td>I havent seen that episode, I might google it...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188375</th>\n",
              "      <td>8628</td>\n",
              "      <td>I don't think I have either. That's an insane...</td>\n",
              "      <td>Curious to dive deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188376</th>\n",
              "      <td>8628</td>\n",
              "      <td>I did, my little brother used to love Thomas ...</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188377</th>\n",
              "      <td>8628</td>\n",
              "      <td>It did. Ringo Starr, George Carlin, and Alec ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188378 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        conversation_id                                            message  \\\n",
              "0                     1              Are you a fan of Google or Microsoft?   \n",
              "1                     1   Both are excellent technology they are helpfu...   \n",
              "2                     1   I'm not  a huge fan of Google, but I use it a...   \n",
              "3                     1   Google provides online related services and p...   \n",
              "4                     1   Yeah, their services are good. I'm just not a...   \n",
              "...                 ...                                                ...   \n",
              "188373             8628   Wow, it does not seem like that long. Since I...   \n",
              "188374             8628   I havent seen that episode, I might google it...   \n",
              "188375             8628   I don't think I have either. That's an insane...   \n",
              "188376             8628   I did, my little brother used to love Thomas ...   \n",
              "188377             8628   It did. Ringo Starr, George Carlin, and Alec ...   \n",
              "\n",
              "                      sentiment  \n",
              "0        Curious to dive deeper  \n",
              "1        Curious to dive deeper  \n",
              "2        Curious to dive deeper  \n",
              "3        Curious to dive deeper  \n",
              "4        Curious to dive deeper  \n",
              "...                         ...  \n",
              "188373                Surprised  \n",
              "188374   Curious to dive deeper  \n",
              "188375   Curious to dive deeper  \n",
              "188376                    Happy  \n",
              "188377                  Neutral  \n",
              "\n",
              "[188378 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#data = pd.read_csv(r'/content/drive/MyDrive/topical_chat.csv')\n",
        "data = pd.read_csv(r'topical_chat.csv')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xi_TaGmK_0Fq"
      },
      "outputs": [],
      "source": [
        "data_arranged = [[] for _ in range(8628)]\n",
        "for i in range(len(data[:])):\n",
        "  data_arranged[data.iloc[i, 0] - 1].append(data.iloc[i, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QS2J_9RaJKds"
      },
      "outputs": [],
      "source": [
        "def truncate_text(data_arrange):\n",
        "  list_data = []\n",
        "  for datas in data_arrange:\n",
        "    datas = np.array([(x + ' [sep]').split() for x in datas], dtype = object)\n",
        "    len_data = np.cumsum([len(x) for x in datas])\n",
        "    filter = np.array(len_data < max_len)\n",
        "    datas = datas[filter]\n",
        "    datas = np.concatenate(datas, axis = 0)\n",
        "    list_data.append(list(datas))\n",
        "  return list_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L8YbWRi3Ulnk"
      },
      "outputs": [],
      "source": [
        "truncated_data = truncate_text(data_arranged)\n",
        "truncated_data = [' '.join(data) for data in truncated_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2iV9KoDg0RKw",
        "outputId": "bb19e615-c9f0-4b88-9e51-40b31dd7cce2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "histogram = list(data['conversation_id'])\n",
        "plt.hist(histogram, bins = list(range(8628)))\n",
        "plt.show()\n",
        "#normal conversation roughly 20 turn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Veyx89Uu3Ilo",
        "outputId": "267665e8-bf2f-4cc4-8def-4cfe759db66d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "conversation = list(data['message'])\n",
        "conversation = [len(x.split()) for x in conversation]\n",
        "plt.hist(conversation, bins = list(range(max(conversation))))\n",
        "plt.show()\n",
        "# so the average len of a conversation is arround 18 word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7mQwjCY9LNC"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ehmRDYK_4ViA"
      },
      "outputs": [],
      "source": [
        "def custom_data(input):\n",
        "  lower = tf.strings.lower(input)\n",
        "  output = tf.strings.regex_replace(lower, '[^a-zA-Z0-9\\s]', '')\n",
        "  return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl8HHQIt-CkS"
      },
      "outputs": [],
      "source": [
        "text_vectorize = keras.layers.TextVectorization(\n",
        "    max_tokens= vocab_size,\n",
        "    standardize = custom_data,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max_len,\n",
        "    pad_to_max_tokens = True,\n",
        "    encoding='utf-8',\n",
        ")\n",
        "text_vectorize.adapt(truncated_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3kuRmjfds5i"
      },
      "outputs": [],
      "source": [
        "def get_mask(array):\n",
        "  mask = array == 0\n",
        "  mask = tf.where(mask == True, 0, 1)\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K1eCcLGZxoY"
      },
      "outputs": [],
      "source": [
        "def preprocess_dataset(text):\n",
        "  data = text_vectorize(text)\n",
        "  mask = get_mask(data)\n",
        "  data = tf.data.Dataset.from_tensor_slices(data)\n",
        "  mask = tf.data.Dataset.from_tensor_slices(mask)\n",
        "  data = tf.data.Dataset.zip((data, mask)).batch(batch_size).shuffle(1000)\n",
        "  data = data.map(lambda data, mask: (data[:, :-1], data[:, 1:], mask[:, :-1], mask[:, 1:]))\n",
        "  data = data.prefetch(tf.data.AUTOTUNE)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-4p_f3owg1A",
        "outputId": "00942476-39b1-4dca-8a49-b62154cd7cee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "dataset = preprocess_dataset(truncated_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG86umikxjRH"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RelativePositionalEncoding(keras.layers.Layer):\n",
        "  def __init__(self, max_relative, num_head):\n",
        "    super().__init__()\n",
        "    self.max_relative = max_relative\n",
        "    self.num_head = num_head\n",
        "    w_init = keras.initializers.GlorotUniform()\n",
        "    with tf.name_scope(self.name):\n",
        "      self.embedding = tf.Variable(\n",
        "          initial_value = w_init(shape=(num_head, max_relative * 2 + 1), dtype=\"float32\"), \n",
        "          trainable = True\n",
        "      )\n",
        "  def call(self, q_len, k_len):\n",
        "    range_vec_q = tf.range(q_len)\n",
        "    range_vec_k = tf.range(k_len)\n",
        "    relative_pos = range_vec_q[:, None] - range_vec_k[None, :]\n",
        "    relative_pos = tf.clip_by_value(relative_pos, -self.max_relative, self.max_relative)\n",
        "    relative_pos = relative_pos + self.max_relative\n",
        "    relative_pos = tf.reshape(tf.tile(relative_pos, [self.num_head, 1]), shape = (self.num_head, q_len, k_len))\n",
        "    num_head_index = tf.range(self.num_head, dtype = 'int32')\n",
        "    num_head_index = tf.reshape(tf.tile(num_head_index, [q_len*k_len]), shape = (q_len, k_len, self.num_head))\n",
        "    num_head_index = tf.transpose(num_head_index, perm = [2, 0, 1]) # shape = num_head, q_len, k_len\n",
        "    relative_pos = tf.stack([num_head_index, relative_pos], axis = 0) # shape = 2, num_head, q_len, k_len\n",
        "    relative_pos = tf.transpose(relative_pos, perm = [1, 2, 3, 0]) # shape = num_head, q_len, k_len, 2\n",
        "    relative_emb = tf.gather_nd(self.embedding, relative_pos) # shape = num_head, q_len, k_len\n",
        "    return relative_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OscPumdSxHg_"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(keras.layers.Layer):\n",
        "  def __init__(self, n_heads = 8, dropout_rate = 0.1, use_usual_case = False, use_relative_embed = False, max_relative_pos = 128, **kwargs):\n",
        "    super(MultiheadAttention, self).__init__()\n",
        "    self.usual = use_usual_case\n",
        "    self.n_heads = n_heads\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.use_relative_embed = use_relative_embed\n",
        "    self.max_relative_pos = max_relative_pos\n",
        "  def build(self, batch_input_shape):\n",
        "    self.batch, self.q_time_steps, self.dim = batch_input_shape\n",
        "    self.q_linear = keras.layers.Conv1D(self.dim * self.n_heads, kernel_size = 1, strides = 1, padding = 'valid', use_bias = False)\n",
        "    self.k_linear = keras.layers.Conv1D(self.dim * self.n_heads, kernel_size = 1, strides = 1, padding = 'valid', use_bias = False)\n",
        "    self.v_linear = keras.layers.Conv1D(self.dim * self.n_heads, kernel_size = 1, strides = 1, padding = 'valid', use_bias = False)\n",
        "    self.linear_out = keras.layers.Conv1D(self.dim, kernel_size = 1, strides = 1, padding = 'valid', use_bias = False)\n",
        "    self.drop_out = keras.layers.Dropout(self.dropout_rate)\n",
        "    if self.use_relative_embed:\n",
        "      self.relative_emb = RelativePositionalEncoding(self.max_relative_pos, self.n_heads)\n",
        "    super().build(batch_input_shape)\n",
        "  def mask_attention(self, q, k, v, masking = None, use_usual_case = False):\n",
        "    q_len = q.shape[1]\n",
        "    k_len = k.shape[1]\n",
        "    if masking is None:\n",
        "      mask = tf.ones(shape = (q_len, k_len))\n",
        "      if use_usual_case:\n",
        "        mask = tf.experimental.numpy.tril(mask, 0)\n",
        "      mask = tf.where(mask == 0, -1e6, 0)\n",
        "    else:\n",
        "      mask = tf.reshape(tf.repeat(masking, q_len * self.n_heads, axis = 0), shape = [-1 , q_len, k_len]) # mask shape = [batch * n_heads, q_len, k_len]\n",
        "      if use_usual_case:\n",
        "        mask = tf.experimental.numpy.tril(mask, 0)\n",
        "      mask = tf.where(mask == 0, -1e6, 0)\n",
        "    normalize = tf.math.sqrt(tf.cast(self.dim, dtype=tf.float32))\n",
        "    attention = tf.linalg.matmul(q, tf.transpose(k, perm = [0, 2, 1])) / normalize\n",
        "    attention = attention + mask\n",
        "    if self.use_relative_embed:\n",
        "      attention = tf.reshape(attention, shape = [-1, self.n_heads, q_len, k_len])\n",
        "      attention = self.relative_emb(q_len, k_len) + attention\n",
        "      attention = tf.reshape(attention, shape = [-1, q_len, k_len])\n",
        "    norm_attention = tf.nn.softmax(attention, axis = -1)\n",
        "    output = tf.linalg.matmul(norm_attention, v)\n",
        "    return output, norm_attention\n",
        "  def linear_attention(self, input, linear):\n",
        "    timestep = input.shape[1]\n",
        "    output = linear(input)\n",
        "    output = tf.reshape(output, shape = [-1, timestep, self.n_heads, self.dim]) # (batch, timestep, n_heads, dimension)\n",
        "    output = tf.transpose(output, perm = [0, 2, 1, 3])\n",
        "    output = tf.reshape(output, shape = [-1 , timestep, self.dim]) # (batch*n_heads, timestep, dimension)\n",
        "    return output\n",
        "  def call(self, Q, V, masking = None):\n",
        "    K = V\n",
        "    Q = self.linear_attention(Q, self.q_linear)\n",
        "    K = self.linear_attention(K, self.k_linear)\n",
        "    V = self.linear_attention(V, self.v_linear)\n",
        "    output, attention_weight = self.mask_attention(Q, K, V, masking = masking, use_usual_case = self.usual)\n",
        "    output = tf.reshape(output, shape = [-1, self.n_heads, self.q_time_steps, self.dim]) # (batch, n_heads, q_time_steps, dimension)\n",
        "    output = tf.transpose(output, perm = [0, 2, 1, 3])\n",
        "    output = tf.reshape(output, shape = [-1, self.q_time_steps, self.n_heads*self.dim]) # (batch, n_heads, q_time_steps, dimension)\n",
        "    output = self.linear_out(output)\n",
        "    output = self.drop_out(output)\n",
        "    return output, attention_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5KoNWhCexson"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
        "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "        pos_emb = np.empty((1, max_steps, max_dims))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6DoRhIKWxupn"
      },
      "outputs": [],
      "source": [
        "class FFN(keras.layers.Layer):\n",
        "  def __init__(self, d_dim = d_dim, drop_rate = drop_rate):\n",
        "    super().__init__()\n",
        "    self.d_dim = d_dim\n",
        "    self.drop = keras.layers.Dropout(drop_rate)\n",
        "    self.normalize = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "  def build(self, batch_input_shape):\n",
        "    self.dense = keras.layers.Dense(self.d_dim, activation = 'relu')\n",
        "    self.d_model = keras.layers.Dense(batch_input_shape[-1])\n",
        "  def call(self, input):\n",
        "    emb = self.dense(input)\n",
        "    bottle_neck = self.d_model(emb)\n",
        "    add = bottle_neck + input\n",
        "    normalize = self.normalize(add)\n",
        "    output = self.drop(normalize)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1Pyag0t-xwMv"
      },
      "outputs": [],
      "source": [
        "class transformer_block(keras.layers.Layer):\n",
        "  def __init__(self, n_heads = n_heads, drop_rate = drop_rate, d_dim = d_dim):\n",
        "    super(transformer_block, self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.drop_rate = drop_rate\n",
        "    self.MulAttention = MultiheadAttention(n_heads, drop_rate, True, True)\n",
        "    #self.MulAttention = keras.layers.MultiHeadAttention(num_heads = n_heads, key_dim = d_dim)\n",
        "    self.ffn = FFN(d_dim, drop_rate)\n",
        "    self.normalize = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "  def call(self, inputs, mask = None):\n",
        "    #x, attention_weight = self.MulAttention(inputs, mask_q, mask_k)\n",
        "    x, _ = self.MulAttention(inputs, inputs, mask)\n",
        "    x = x + inputs\n",
        "    normalize = self.normalize(x)\n",
        "    output = self.ffn(normalize)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S03qJ95Jxx0P"
      },
      "outputs": [],
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "class MaskLanguageModel(keras.Model):\n",
        "  def __init__(self, d_dim = d_dim, vocab_size = vocab_size ,max_steps = max_len, max_dims = max_dim, n_heads = n_heads, drop_rate = drop_rate, n_blocks = n_blocks):\n",
        "    super().__init__()\n",
        "    self.position = PositionalEncoding(max_steps = max_steps, max_dims = max_dims)\n",
        "    self.transformers = [transformer_block(n_heads = n_heads, drop_rate = drop_rate, d_dim = d_dim) for _ in range(n_blocks)]\n",
        "    self.dense = keras.layers.Dense(vocab_size)\n",
        "    self.embed = keras.layers.Embedding(vocab_size, max_dims, input_length = max_steps)\n",
        "  def call(self, inputs, mask = None):\n",
        "    embed = self.embed(inputs)\n",
        "    pos_emb = self.position(embed)\n",
        "    x = self.transformers[0](pos_emb, mask)\n",
        "    for block in self.transformers[1:]:\n",
        "      x = block(x, mask)\n",
        "    output = self.dense(x)\n",
        "    output = tf.nn.softmax(output, axis = -1)\n",
        "    return output\n",
        "  def train_step(self, data):\n",
        "    input, label, mask_input, mask_label = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(input, mask_input)\n",
        "      loss = loss_fn(label, predictions, sample_weight = mask_label)\n",
        "    # Compute gradients\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "    # Compute our own metrics\n",
        "    loss_tracker.update_state(loss, sample_weight = mask_label)\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {\"loss\": loss_tracker.result()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8P0DDGez1sB"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 3)\n",
        "model = MaskLanguageModel(max_steps = max_len - 1)\n",
        "model.compile(optimizer = keras.optimizers.Adam())\n",
        "history = model.fit(dataset, epochs = 30, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fA3_H1hc6l3C"
      },
      "outputs": [],
      "source": [
        "vocab = text_vectorize.get_vocabulary()\n",
        "def decode_sentence(array):\n",
        "  return \" \".join([vocab[each] for each in array])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Qu-eTDbfpg7E"
      },
      "outputs": [],
      "source": [
        "def inference(text):\n",
        "  text += ' sep'\n",
        "  index = len(text.split())\n",
        "  list_prediction = []\n",
        "  prediction = ''\n",
        "  while prediction != 'sep':\n",
        "    encode_text = text_vectorize([text])[:, :-1]\n",
        "    mask = get_mask(encode_text)\n",
        "    prediction = model(encode_text, mask)\n",
        "    prediction = np.argmax(prediction, axis = -1)[0]\n",
        "    prediction = vocab[prediction[index]]\n",
        "    text += ' ' + prediction\n",
        "    list_prediction.append(prediction)\n",
        "    index += 1\n",
        "  return ' '.join(list_prediction[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QRzo06M05P2J",
        "outputId": "003d4c60-b3c0-4dc9-844c-f9756a1928b1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'was and i i like too and i like too i and my is a fan'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inference('Hey what\\'s up do use Google very often?I really love the company and was surprised to hear that it was founded back in 1998.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui6fSRrKzk1U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO8n6MiQz/5gcVIxRX7PGm0",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "14c0cc5a2914ae78849257d74efb3e5392f9050abd9340a58c7e45cf8b1836b5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
